{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b279932c-db58-46f2-95ee-2f5c57b4bf4d",
   "metadata": {},
   "source": [
    "# Notebook to train binary classifier (before, after growth plate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b3cdf0e-adaf-48e3-98ba-787051bd791a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from multiprocessing import freeze_support\n",
    "from torchvision.transforms import v2\n",
    "from data_utils.dataset import BoneSlicesDatasetPrev\n",
    "from training_utils import train_one_epoch\n",
    "from validation_metrics import calculate_metric\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import precision_score\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch import nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ce80b-2760-4d28-aa63-704d8ebce2e9",
   "metadata": {},
   "source": [
    "## Augmentations (for future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f438a3-c6ed-46e0-9b40-877c875d2d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#v2.Resize(size=(224, 224), antialias=True),\n",
    "transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(0.5),\n",
    "    v2.RandomVerticalFlip(0.5),\n",
    "    v2.RandomRotation(degrees=180),\n",
    "    v2.ToDtype(torch.float32, scale=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b66edef-9f33-492f-bf72-c0311d8c1f4a",
   "metadata": {},
   "source": [
    "## Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145d16ac-a637-41ed-bc8d-402b6932139f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "TIMESTAMP: 20240520_093559\n",
      "CURRENT WORKING DIR: /home/ec2-user/SageMaker/code-warriors-imgcollab/training\n",
      "DEVICE: cuda\n",
      "BASE_DIR: training/resnet_18_6_all/\n",
      "WRITER_DIR: training/resnet_18_6_all/logs\n",
      "MODEL_PATH: training/resnet_18_6_all/saved_models/\n",
      "BATCH_SIZE: 64\n",
      "NUM_WORKERS: 4\n",
      "LEARNING_RATE: 0.0001\n",
      "EPOCHS: 20\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else 'cpu'\n",
    "DEVIE = 'cpu'\n",
    "BASE_DIR = 'training/resnet_18_6_all/'\n",
    "WRITER_DIR = BASE_DIR + \"logs\"\n",
    "MODEL_PATH = BASE_DIR + \"saved_models/\"\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "LEARNING_RATE = 0.0001\n",
    "EPOCHS = 20\n",
    "\n",
    "# os.chdir('..')\n",
    "print(\"--------------------\")\n",
    "print(f\"TIMESTAMP: {TIMESTAMP}\")\n",
    "\n",
    "print(f\"CURRENT WORKING DIR: {os.getcwd()}\") \n",
    "print(f\"DEVICE: {DEVICE}\")\n",
    "print(f\"BASE_DIR: {BASE_DIR}\")\n",
    "print(f\"WRITER_DIR: {WRITER_DIR}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")\n",
    "print(f\"BATCH_SIZE: {BATCH_SIZE}\")\n",
    "print(f\"NUM_WORKERS: {NUM_WORKERS}\")\n",
    "print(f\"LEARNING_RATE: {LEARNING_RATE}\")\n",
    "print(f\"EPOCHS: {EPOCHS}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55991925-eec9-4801-9a41-b29fce6c457f",
   "metadata": {},
   "source": [
    "## Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f02302-33dd-4f9d-bf2e-d71f9374d99c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/code-warriors-imgcollab\n"
     ]
    }
   ],
   "source": [
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26457fa0-66d4-4ee5-8037-514695f6554f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "METADATA_PATH = os.path.join('train.csv')\n",
    "metadata_df = pd.read_csv(METADATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de0b919-7a86-4d31-af8f-2bbcb4001537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_balanced_dataset(meta_df, column, fraction=0.2):\n",
    "    validation_examples = []\n",
    "    for val in meta_df[column].unique():\n",
    "        subdf = meta_df[meta_df[column] == val]\n",
    "        validation_examples += list(subdf.sample(frac=fraction)['Image Name'])\n",
    "    training_examples = set(meta_df['Image Name'].unique()).difference(set(validation_examples))\n",
    "    return validation_examples, list(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67644567-3102-467f-802c-885d0b27faab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validation_examples, training_examples = get_balanced_dataset(metadata_df, 'STUDY ID', fraction=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ff3a73-e798-4ac9-ac28-e200e1365633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VALIDATION_EXAMPLES_FILE = '10folds/5_fold/validation_examples.csv'\n",
    "# TRAINING_EXAMPLES_FILE = '10folds/5_fold/training_examples.csv'\n",
    "# # val_df = pd.DataFrame(validation_examples, columns=['Image Name'])\n",
    "# # train_df = pd.DataFrame(training_examples, columns=['Image Name'])\n",
    "# # val_df.to_csv(VALIDATION_EXAMPLES_FILE)\n",
    "# # train_df.to_csv(TRAINING_EXAMPLES_FILE)\n",
    "# validation_examples = list(pd.read_csv(VALIDATION_EXAMPLES_FILE)['Image Name'])\n",
    "# training_examples = list(pd.read_csv(TRAINING_EXAMPLES_FILE)['Image Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7c6c7a-053c-4fda-806b-72bba180d8f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44940\n",
      "44940\n"
     ]
    }
   ],
   "source": [
    "train_ds = BoneSlicesDatasetPrev(json_config_filepath = 'data_utils/config_binary_z.json', transform=transforms)\n",
    "valid_ds = BoneSlicesDatasetPrev(json_config_filepath = 'data_utils/config_binary_z.json')\n",
    "# train_ds.subset_by_image_name(training_examples)\n",
    "# valid_ds.subset_by_image_name(validation_examples)\n",
    "print(len(valid_ds.metadata['Image Name']))\n",
    "\n",
    "print(len(train_ds.metadata['Image Name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9329488-9add-417f-9090-075e700ba0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>STUDY ID</th>\n",
       "      <th>Bone ID</th>\n",
       "      <th>Image Name</th>\n",
       "      <th>Growth Plate Index</th>\n",
       "      <th>Slice Index</th>\n",
       "      <th>img_file_name</th>\n",
       "      <th>axis</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32c1aa1bcd</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>32c1aa1bcd_0.npy</td>\n",
       "      <td>z</td>\n",
       "      <td>sliced_data_new/z_axis/32c1aa1bcd_0.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32c1aa1bcd</td>\n",
       "      <td>168</td>\n",
       "      <td>1</td>\n",
       "      <td>32c1aa1bcd_1.npy</td>\n",
       "      <td>z</td>\n",
       "      <td>sliced_data_new/z_axis/32c1aa1bcd_1.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32c1aa1bcd</td>\n",
       "      <td>168</td>\n",
       "      <td>2</td>\n",
       "      <td>32c1aa1bcd_2.npy</td>\n",
       "      <td>z</td>\n",
       "      <td>sliced_data_new/z_axis/32c1aa1bcd_2.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32c1aa1bcd</td>\n",
       "      <td>168</td>\n",
       "      <td>3</td>\n",
       "      <td>32c1aa1bcd_3.npy</td>\n",
       "      <td>z</td>\n",
       "      <td>sliced_data_new/z_axis/32c1aa1bcd_3.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>32c1aa1bcd</td>\n",
       "      <td>168</td>\n",
       "      <td>4</td>\n",
       "      <td>32c1aa1bcd_4.npy</td>\n",
       "      <td>z</td>\n",
       "      <td>sliced_data_new/z_axis/32c1aa1bcd_4.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0  STUDY ID  Bone ID  Image Name  Growth Plate Index  \\\n",
       "0      0           0         0        4  32c1aa1bcd                 168   \n",
       "1      1           1         0        4  32c1aa1bcd                 168   \n",
       "2      2           2         0        4  32c1aa1bcd                 168   \n",
       "3      3           3         0        4  32c1aa1bcd                 168   \n",
       "4      4           4         0        4  32c1aa1bcd                 168   \n",
       "\n",
       "   Slice Index     img_file_name axis                                 img_path  \n",
       "0            0  32c1aa1bcd_0.npy    z  sliced_data_new/z_axis/32c1aa1bcd_0.npy  \n",
       "1            1  32c1aa1bcd_1.npy    z  sliced_data_new/z_axis/32c1aa1bcd_1.npy  \n",
       "2            2  32c1aa1bcd_2.npy    z  sliced_data_new/z_axis/32c1aa1bcd_2.npy  \n",
       "3            3  32c1aa1bcd_3.npy    z  sliced_data_new/z_axis/32c1aa1bcd_3.npy  \n",
       "4            4  32c1aa1bcd_4.npy    z  sliced_data_new/z_axis/32c1aa1bcd_4.npy  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ds.metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968c7b83-ca7d-4f21-b819-ce03f703f60d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl = DataLoader(valid_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3639ff-f4fd-45fb-8fd8-192f7a84681f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d5183-9cad-4d43-9df1-b2f862a4b53f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##############################################\n",
      "Iteration: 1\n",
      "##############################################\n",
      "\n",
      "EPOCH 1:\n",
      "Loss after batch 10: 0.2524; time elapsed: 9.97\n",
      "Loss after batch 20: 0.1245; time elapsed: 18.44\n",
      "Loss after batch 30: 0.0986; time elapsed: 27.39\n",
      "Loss after batch 40: 0.0872; time elapsed: 35.95\n",
      "Loss after batch 50: 0.0658; time elapsed: 43.97\n",
      "Loss after batch 60: 0.0368; time elapsed: 52.15\n",
      "Loss after batch 70: 0.0355; time elapsed: 60.54\n",
      "Loss after batch 80: 0.0301; time elapsed: 68.85\n",
      "Loss after batch 90: 0.0353; time elapsed: 76.90\n",
      "Loss after batch 100: 0.0365; time elapsed: 84.97\n",
      "Loss after batch 110: 0.0384; time elapsed: 93.14\n",
      "Loss after batch 120: 0.0458; time elapsed: 101.26\n",
      "Loss after batch 130: 0.0304; time elapsed: 109.26\n",
      "Loss after batch 140: 0.0257; time elapsed: 117.24\n",
      "Loss after batch 150: 0.0361; time elapsed: 125.56\n",
      "Loss after batch 160: 0.0348; time elapsed: 133.84\n",
      "Loss after batch 170: 0.0581; time elapsed: 141.97\n",
      "Loss after batch 180: 0.0402; time elapsed: 150.42\n",
      "Loss after batch 190: 0.0451; time elapsed: 158.63\n",
      "Loss after batch 200: 0.0328; time elapsed: 166.61\n",
      "Loss after batch 210: 0.0386; time elapsed: 174.78\n",
      "Loss after batch 220: 0.0138; time elapsed: 183.29\n",
      "Loss after batch 230: 0.0368; time elapsed: 191.40\n",
      "Loss after batch 240: 0.0308; time elapsed: 199.48\n",
      "Loss after batch 250: 0.0404; time elapsed: 207.49\n",
      "Loss after batch 260: 0.0206; time elapsed: 215.90\n",
      "Loss after batch 270: 0.0267; time elapsed: 223.93\n",
      "Loss after batch 280: 0.0240; time elapsed: 231.99\n",
      "Loss after batch 290: 0.0312; time elapsed: 240.15\n",
      "Loss after batch 300: 0.0293; time elapsed: 248.39\n",
      "Loss after batch 310: 0.0146; time elapsed: 256.74\n",
      "Loss after batch 320: 0.0313; time elapsed: 264.82\n",
      "Loss after batch 330: 0.0285; time elapsed: 273.52\n",
      "Loss after batch 340: 0.0466; time elapsed: 282.01\n",
      "Loss after batch 350: 0.0441; time elapsed: 290.33\n",
      "Loss after batch 360: 0.0281; time elapsed: 298.39\n",
      "Loss after batch 370: 0.0222; time elapsed: 306.63\n",
      "Loss after batch 380: 0.0338; time elapsed: 314.79\n",
      "Loss after batch 390: 0.0184; time elapsed: 322.88\n",
      "Loss after batch 400: 0.0258; time elapsed: 330.97\n",
      "Loss after batch 410: 0.0184; time elapsed: 339.10\n",
      "Loss after batch 420: 0.0175; time elapsed: 347.12\n",
      "Loss after batch 430: 0.0151; time elapsed: 355.19\n",
      "Loss after batch 440: 0.0538; time elapsed: 363.76\n",
      "Loss after batch 450: 0.0291; time elapsed: 371.86\n",
      "Loss after batch 460: 0.0286; time elapsed: 380.13\n",
      "Loss after batch 470: 0.0207; time elapsed: 388.38\n",
      "Loss after batch 480: 0.0521; time elapsed: 396.68\n",
      "Loss after batch 490: 0.0370; time elapsed: 404.93\n",
      "Loss after batch 500: 0.0242; time elapsed: 413.25\n",
      "Loss after batch 510: 0.0266; time elapsed: 421.96\n",
      "Loss after batch 520: 0.0212; time elapsed: 430.31\n",
      "Loss after batch 530: 0.0299; time elapsed: 438.51\n",
      "Loss after batch 540: 0.0115; time elapsed: 446.63\n",
      "Loss after batch 550: 0.0218; time elapsed: 455.21\n",
      "Loss after batch 560: 0.0215; time elapsed: 463.30\n",
      "Loss after batch 570: 0.0099; time elapsed: 471.40\n",
      "Loss after batch 580: 0.0260; time elapsed: 479.50\n",
      "Loss after batch 590: 0.0245; time elapsed: 487.64\n",
      "Loss after batch 600: 0.0145; time elapsed: 495.80\n",
      "Loss after batch 610: 0.0348; time elapsed: 504.87\n",
      "Loss after batch 620: 0.0292; time elapsed: 513.38\n",
      "Loss after batch 630: 0.0179; time elapsed: 521.57\n",
      "Loss after batch 640: 0.0440; time elapsed: 529.70\n",
      "Loss after batch 650: 0.0404; time elapsed: 538.30\n",
      "Loss after batch 660: 0.0328; time elapsed: 546.69\n",
      "Loss after batch 670: 0.0376; time elapsed: 554.94\n",
      "Loss after batch 680: 0.0141; time elapsed: 563.15\n",
      "Loss after batch 690: 0.0212; time elapsed: 571.31\n",
      "Loss after batch 700: 0.0301; time elapsed: 579.49\n",
      "#############################################################\n",
      "Epoch results:\n",
      "Loss train 0.0300849718216341 valid loss: 0.01720643788576126\n",
      "Validation macro average precision: 0.9903104472367146\n",
      "Epoch execution time 1123.870966911316\n",
      "#############################################################\n",
      "\n",
      "\n",
      "EPOCH 2:\n",
      "Loss after batch 10: 0.0117; time elapsed: 1131.47\n",
      "Loss after batch 20: 0.0173; time elapsed: 1138.94\n",
      "Loss after batch 30: 0.0185; time elapsed: 1146.55\n",
      "Loss after batch 40: 0.0383; time elapsed: 1154.23\n",
      "Loss after batch 50: 0.0243; time elapsed: 1161.86\n",
      "Loss after batch 60: 0.0230; time elapsed: 1169.84\n",
      "Loss after batch 70: 0.0200; time elapsed: 1177.79\n",
      "Loss after batch 80: 0.0158; time elapsed: 1185.55\n",
      "Loss after batch 90: 0.0193; time elapsed: 1193.36\n",
      "Loss after batch 100: 0.0316; time elapsed: 1201.24\n",
      "Loss after batch 110: 0.0169; time elapsed: 1209.23\n",
      "Loss after batch 120: 0.0238; time elapsed: 1217.20\n",
      "Loss after batch 130: 0.0125; time elapsed: 1225.72\n",
      "Loss after batch 140: 0.0130; time elapsed: 1234.10\n",
      "Loss after batch 150: 0.0099; time elapsed: 1242.77\n",
      "Loss after batch 160: 0.0213; time elapsed: 1250.78\n",
      "Loss after batch 170: 0.0141; time elapsed: 1259.39\n",
      "Loss after batch 180: 0.0151; time elapsed: 1268.38\n",
      "Loss after batch 190: 0.0144; time elapsed: 1276.60\n",
      "Loss after batch 200: 0.0377; time elapsed: 1284.82\n",
      "Loss after batch 210: 0.0244; time elapsed: 1293.13\n",
      "Loss after batch 220: 0.0217; time elapsed: 1301.50\n",
      "Loss after batch 230: 0.0262; time elapsed: 1309.56\n",
      "Loss after batch 240: 0.0250; time elapsed: 1317.58\n",
      "Loss after batch 250: 0.0134; time elapsed: 1325.66\n",
      "Loss after batch 260: 0.0234; time elapsed: 1333.91\n",
      "Loss after batch 270: 0.0247; time elapsed: 1342.22\n",
      "Loss after batch 280: 0.0209; time elapsed: 1350.56\n",
      "Loss after batch 290: 0.0295; time elapsed: 1359.00\n",
      "Loss after batch 300: 0.0286; time elapsed: 1367.35\n",
      "Loss after batch 310: 0.0167; time elapsed: 1375.57\n",
      "Loss after batch 320: 0.0198; time elapsed: 1383.83\n",
      "Loss after batch 330: 0.0323; time elapsed: 1391.97\n",
      "Loss after batch 340: 0.0274; time elapsed: 1400.01\n",
      "Loss after batch 350: 0.0187; time elapsed: 1408.27\n",
      "Loss after batch 360: 0.0229; time elapsed: 1416.45\n",
      "Loss after batch 370: 0.0245; time elapsed: 1424.77\n",
      "Loss after batch 380: 0.0229; time elapsed: 1432.85\n",
      "Loss after batch 390: 0.0208; time elapsed: 1441.40\n",
      "Loss after batch 400: 0.0417; time elapsed: 1449.71\n",
      "Loss after batch 410: 0.0121; time elapsed: 1457.76\n",
      "Loss after batch 420: 0.0102; time elapsed: 1465.93\n",
      "Loss after batch 430: 0.0135; time elapsed: 1474.88\n",
      "Loss after batch 440: 0.0154; time elapsed: 1483.88\n",
      "Loss after batch 450: 0.0178; time elapsed: 1492.20\n",
      "Loss after batch 460: 0.0185; time elapsed: 1501.00\n",
      "Loss after batch 470: 0.0192; time elapsed: 1509.37\n",
      "Loss after batch 480: 0.0156; time elapsed: 1517.56\n",
      "Loss after batch 490: 0.0108; time elapsed: 1525.84\n",
      "Loss after batch 500: 0.0087; time elapsed: 1534.40\n",
      "Loss after batch 510: 0.0195; time elapsed: 1543.07\n",
      "Loss after batch 520: 0.0116; time elapsed: 1551.44\n",
      "Loss after batch 530: 0.0146; time elapsed: 1559.85\n",
      "Loss after batch 540: 0.0113; time elapsed: 1568.24\n",
      "Loss after batch 550: 0.0221; time elapsed: 1576.37\n",
      "Loss after batch 560: 0.0109; time elapsed: 1584.53\n",
      "Loss after batch 570: 0.0197; time elapsed: 1593.19\n",
      "Loss after batch 580: 0.0130; time elapsed: 1601.98\n",
      "Loss after batch 590: 0.0113; time elapsed: 1611.19\n",
      "Loss after batch 600: 0.0204; time elapsed: 1619.37\n",
      "Loss after batch 610: 0.0124; time elapsed: 1627.88\n",
      "Loss after batch 620: 0.0170; time elapsed: 1635.95\n",
      "Loss after batch 630: 0.0135; time elapsed: 1644.33\n",
      "Loss after batch 640: 0.0282; time elapsed: 1652.53\n",
      "Loss after batch 650: 0.0090; time elapsed: 1660.85\n",
      "Loss after batch 660: 0.0440; time elapsed: 1669.20\n",
      "Loss after batch 670: 0.0207; time elapsed: 1677.21\n",
      "Loss after batch 680: 0.0289; time elapsed: 1685.63\n",
      "Loss after batch 690: 0.0214; time elapsed: 1693.98\n",
      "Loss after batch 700: 0.0202; time elapsed: 1702.28\n",
      "#############################################################\n",
      "Epoch results:\n",
      "Loss train 0.02020974620245397 valid loss: 0.019539620727300644\n",
      "Validation macro average precision: 0.9856218553917602\n",
      "Epoch execution time 1133.9838922023773\n",
      "#############################################################\n",
      "\n",
      "\n",
      "EPOCH 3:\n",
      "Loss after batch 10: 0.0251; time elapsed: 2265.74\n",
      "Loss after batch 20: 0.0067; time elapsed: 2273.33\n",
      "Loss after batch 30: 0.0222; time elapsed: 2281.74\n",
      "Loss after batch 40: 0.0349; time elapsed: 2290.00\n",
      "Loss after batch 50: 0.0066; time elapsed: 2297.60\n",
      "Loss after batch 60: 0.0196; time elapsed: 2305.30\n",
      "Loss after batch 70: 0.0186; time elapsed: 2313.45\n",
      "Loss after batch 80: 0.0178; time elapsed: 2321.50\n",
      "Loss after batch 90: 0.0080; time elapsed: 2329.72\n",
      "Loss after batch 100: 0.0116; time elapsed: 2337.85\n",
      "Loss after batch 110: 0.0257; time elapsed: 2347.07\n",
      "Loss after batch 120: 0.0197; time elapsed: 2355.87\n",
      "Loss after batch 130: 0.0168; time elapsed: 2364.48\n",
      "Loss after batch 140: 0.0183; time elapsed: 2373.38\n",
      "Loss after batch 150: 0.0111; time elapsed: 2382.22\n",
      "Loss after batch 160: 0.0100; time elapsed: 2390.85\n",
      "Loss after batch 170: 0.0213; time elapsed: 2399.12\n",
      "Loss after batch 180: 0.0188; time elapsed: 2407.97\n",
      "Loss after batch 190: 0.0167; time elapsed: 2416.74\n",
      "Loss after batch 200: 0.0188; time elapsed: 2425.53\n",
      "Loss after batch 210: 0.0165; time elapsed: 2434.77\n",
      "Loss after batch 220: 0.0038; time elapsed: 2444.22\n",
      "Loss after batch 230: 0.0233; time elapsed: 2452.93\n",
      "Loss after batch 240: 0.0427; time elapsed: 2461.16\n",
      "Loss after batch 250: 0.0246; time elapsed: 2469.49\n",
      "Loss after batch 260: 0.0211; time elapsed: 2477.79\n",
      "Loss after batch 270: 0.0236; time elapsed: 2485.93\n",
      "Loss after batch 280: 0.0291; time elapsed: 2494.02\n",
      "Loss after batch 290: 0.0181; time elapsed: 2502.23\n",
      "Loss after batch 300: 0.0186; time elapsed: 2510.74\n",
      "Loss after batch 310: 0.0220; time elapsed: 2519.70\n",
      "Loss after batch 320: 0.0217; time elapsed: 2529.00\n",
      "Loss after batch 330: 0.0151; time elapsed: 2537.91\n",
      "Loss after batch 340: 0.0203; time elapsed: 2546.74\n",
      "Loss after batch 350: 0.0153; time elapsed: 2555.85\n",
      "Loss after batch 360: 0.0114; time elapsed: 2565.75\n",
      "Loss after batch 370: 0.0105; time elapsed: 2574.84\n",
      "Loss after batch 380: 0.0190; time elapsed: 2583.24\n",
      "Loss after batch 390: 0.0164; time elapsed: 2591.65\n",
      "Loss after batch 400: 0.0139; time elapsed: 2600.17\n",
      "Loss after batch 410: 0.0233; time elapsed: 2608.29\n",
      "Loss after batch 420: 0.0145; time elapsed: 2616.47\n",
      "Loss after batch 430: 0.0223; time elapsed: 2624.81\n",
      "Loss after batch 440: 0.0155; time elapsed: 2633.31\n",
      "Loss after batch 450: 0.0165; time elapsed: 2641.81\n",
      "Loss after batch 460: 0.0136; time elapsed: 2650.17\n",
      "Loss after batch 470: 0.0237; time elapsed: 2658.45\n",
      "Loss after batch 480: 0.0107; time elapsed: 2666.51\n",
      "Loss after batch 490: 0.0198; time elapsed: 2675.09\n",
      "Loss after batch 500: 0.0095; time elapsed: 2683.95\n",
      "Loss after batch 510: 0.0062; time elapsed: 2692.60\n",
      "Loss after batch 520: 0.0046; time elapsed: 2701.09\n",
      "Loss after batch 530: 0.0176; time elapsed: 2709.62\n",
      "Loss after batch 540: 0.0271; time elapsed: 2718.27\n",
      "Loss after batch 550: 0.0129; time elapsed: 2726.38\n",
      "Loss after batch 560: 0.0101; time elapsed: 2734.84\n",
      "Loss after batch 570: 0.0102; time elapsed: 2743.15\n",
      "Loss after batch 580: 0.0243; time elapsed: 2751.28\n",
      "Loss after batch 590: 0.0286; time elapsed: 2759.54\n",
      "Loss after batch 600: 0.0186; time elapsed: 2767.94\n",
      "Loss after batch 610: 0.0217; time elapsed: 2776.20\n",
      "Loss after batch 620: 0.0179; time elapsed: 2784.79\n",
      "Loss after batch 630: 0.0194; time elapsed: 2793.45\n",
      "Loss after batch 640: 0.0146; time elapsed: 2802.49\n",
      "Loss after batch 650: 0.0112; time elapsed: 2811.77\n",
      "Loss after batch 660: 0.0142; time elapsed: 2820.60\n",
      "Loss after batch 670: 0.0361; time elapsed: 2829.60\n",
      "Loss after batch 680: 0.0146; time elapsed: 2838.00\n",
      "Loss after batch 690: 0.0077; time elapsed: 2846.05\n",
      "Loss after batch 700: 0.0257; time elapsed: 2854.26\n",
      "#############################################################\n",
      "Epoch results:\n",
      "Loss train 0.02570405607111752 valid loss: 0.011483694426715374\n",
      "Validation macro average precision: 0.9939795126108905\n",
      "Epoch execution time 1145.131341934204\n",
      "#############################################################\n",
      "\n",
      "\n",
      "EPOCH 4:\n",
      "Loss after batch 10: 0.0087; time elapsed: 3411.46\n",
      "Loss after batch 20: 0.0076; time elapsed: 3419.22\n",
      "Loss after batch 30: 0.0062; time elapsed: 3427.29\n",
      "Loss after batch 40: 0.0315; time elapsed: 3435.08\n",
      "Loss after batch 50: 0.0217; time elapsed: 3442.71\n",
      "Loss after batch 60: 0.0097; time elapsed: 3450.38\n",
      "Loss after batch 70: 0.0193; time elapsed: 3458.09\n",
      "Loss after batch 80: 0.0064; time elapsed: 3465.80\n",
      "Loss after batch 90: 0.0120; time elapsed: 3473.63\n",
      "Loss after batch 100: 0.0139; time elapsed: 3482.22\n",
      "Loss after batch 110: 0.0085; time elapsed: 3490.41\n",
      "Loss after batch 120: 0.0276; time elapsed: 3498.62\n",
      "Loss after batch 130: 0.0174; time elapsed: 3506.89\n",
      "Loss after batch 140: 0.0219; time elapsed: 3515.16\n",
      "Loss after batch 150: 0.0154; time elapsed: 3523.19\n",
      "Loss after batch 160: 0.0212; time elapsed: 3531.88\n",
      "Loss after batch 170: 0.0124; time elapsed: 3540.09\n",
      "Loss after batch 180: 0.0116; time elapsed: 3548.63\n",
      "Loss after batch 190: 0.0071; time elapsed: 3557.28\n",
      "Loss after batch 200: 0.0135; time elapsed: 3565.46\n",
      "Loss after batch 210: 0.0158; time elapsed: 3573.78\n",
      "Loss after batch 220: 0.0219; time elapsed: 3582.03\n",
      "Loss after batch 230: 0.0152; time elapsed: 3590.08\n",
      "Loss after batch 240: 0.0091; time elapsed: 3598.70\n",
      "Loss after batch 250: 0.0145; time elapsed: 3607.27\n",
      "Loss after batch 260: 0.0075; time elapsed: 3615.54\n",
      "Loss after batch 270: 0.0094; time elapsed: 3623.81\n",
      "Loss after batch 280: 0.0106; time elapsed: 3632.01\n",
      "Loss after batch 290: 0.0066; time elapsed: 3640.27\n",
      "Loss after batch 300: 0.0047; time elapsed: 3648.80\n",
      "Loss after batch 310: 0.0219; time elapsed: 3657.56\n",
      "Loss after batch 320: 0.0132; time elapsed: 3665.96\n",
      "Loss after batch 330: 0.0137; time elapsed: 3674.08\n",
      "Loss after batch 340: 0.0637; time elapsed: 3682.74\n",
      "Loss after batch 350: 0.0775; time elapsed: 3690.90\n",
      "Loss after batch 360: 0.0372; time elapsed: 3699.13\n",
      "Loss after batch 370: 0.0242; time elapsed: 3707.26\n",
      "Loss after batch 380: 0.0208; time elapsed: 3715.29\n",
      "Loss after batch 390: 0.0386; time elapsed: 3723.49\n",
      "Loss after batch 400: 0.0151; time elapsed: 3731.74\n",
      "Loss after batch 410: 0.0328; time elapsed: 3739.86\n",
      "Loss after batch 420: 0.0189; time elapsed: 3748.05\n",
      "Loss after batch 430: 0.0135; time elapsed: 3756.54\n",
      "Loss after batch 440: 0.0129; time elapsed: 3764.54\n",
      "Loss after batch 450: 0.0255; time elapsed: 3773.32\n",
      "Loss after batch 460: 0.0205; time elapsed: 3783.14\n",
      "Loss after batch 470: 0.0121; time elapsed: 3791.93\n",
      "Loss after batch 480: 0.0199; time elapsed: 3801.04\n",
      "Loss after batch 490: 0.0150; time elapsed: 3809.93\n",
      "Loss after batch 500: 0.0219; time elapsed: 3819.11\n",
      "Loss after batch 510: 0.0111; time elapsed: 3827.91\n",
      "Loss after batch 520: 0.0147; time elapsed: 3836.82\n",
      "Loss after batch 530: 0.0070; time elapsed: 3845.12\n",
      "Loss after batch 540: 0.0029; time elapsed: 3853.20\n",
      "Loss after batch 550: 0.0158; time elapsed: 3861.27\n",
      "Loss after batch 560: 0.0182; time elapsed: 3869.52\n",
      "Loss after batch 570: 0.0060; time elapsed: 3878.05\n",
      "Loss after batch 580: 0.0072; time elapsed: 3886.19\n",
      "Loss after batch 590: 0.0167; time elapsed: 3894.87\n",
      "Loss after batch 600: 0.0135; time elapsed: 3903.37\n",
      "Loss after batch 610: 0.0176; time elapsed: 3911.56\n",
      "Loss after batch 620: 0.0207; time elapsed: 3920.65\n",
      "Loss after batch 630: 0.0237; time elapsed: 3929.77\n",
      "Loss after batch 640: 0.0130; time elapsed: 3938.83\n",
      "Loss after batch 650: 0.0232; time elapsed: 3947.98\n",
      "Loss after batch 660: 0.0142; time elapsed: 3956.74\n",
      "Loss after batch 670: 0.0091; time elapsed: 3965.88\n",
      "Loss after batch 680: 0.0207; time elapsed: 3974.90\n",
      "Loss after batch 690: 0.0087; time elapsed: 3983.91\n",
      "Loss after batch 700: 0.0080; time elapsed: 3992.74\n",
      "#############################################################\n",
      "Epoch results:\n",
      "Loss train 0.008046222453049267 valid loss: 0.011611600406467915\n",
      "Validation macro average precision: 0.9926253473779435\n",
      "Epoch execution time 1157.1719222068787\n",
      "#############################################################\n",
      "\n",
      "\n",
      "EPOCH 5:\n",
      "Loss after batch 10: 0.0186; time elapsed: 4571.20\n",
      "Loss after batch 20: 0.0202; time elapsed: 4581.68\n",
      "Loss after batch 30: 0.0023; time elapsed: 4591.44\n",
      "Loss after batch 40: 0.0151; time elapsed: 4601.86\n",
      "Loss after batch 50: 0.0087; time elapsed: 4612.22\n",
      "Loss after batch 60: 0.0132; time elapsed: 4622.33\n",
      "Loss after batch 70: 0.0070; time elapsed: 4632.75\n",
      "Loss after batch 80: 0.0099; time elapsed: 4642.84\n",
      "Loss after batch 90: 0.0313; time elapsed: 4653.11\n",
      "Loss after batch 100: 0.0191; time elapsed: 4663.21\n",
      "Loss after batch 110: 0.0082; time elapsed: 4672.26\n",
      "Loss after batch 120: 0.0057; time elapsed: 4680.77\n",
      "Loss after batch 130: 0.0203; time elapsed: 4689.20\n",
      "Loss after batch 140: 0.0296; time elapsed: 4697.40\n",
      "Loss after batch 150: 0.0177; time elapsed: 4705.49\n",
      "Loss after batch 160: 0.0108; time elapsed: 4713.55\n",
      "Loss after batch 170: 0.0030; time elapsed: 4721.66\n",
      "Loss after batch 180: 0.0053; time elapsed: 4730.08\n",
      "Loss after batch 190: 0.0126; time elapsed: 4738.65\n",
      "Loss after batch 200: 0.0086; time elapsed: 4747.30\n",
      "Loss after batch 210: 0.0136; time elapsed: 4755.92\n",
      "Loss after batch 220: 0.0177; time elapsed: 4763.89\n",
      "Loss after batch 230: 0.0098; time elapsed: 4771.91\n",
      "Loss after batch 240: 0.0099; time elapsed: 4780.04\n",
      "Loss after batch 250: 0.0111; time elapsed: 4788.16\n",
      "Loss after batch 260: 0.0271; time elapsed: 4796.34\n",
      "Loss after batch 270: 0.0101; time elapsed: 4804.33\n",
      "Loss after batch 280: 0.0197; time elapsed: 4812.36\n",
      "Loss after batch 290: 0.0089; time elapsed: 4820.66\n",
      "Loss after batch 300: 0.0029; time elapsed: 4828.70\n",
      "Loss after batch 310: 0.0075; time elapsed: 4836.68\n",
      "Loss after batch 320: 0.0094; time elapsed: 4845.11\n",
      "Loss after batch 330: 0.0242; time elapsed: 4853.67\n",
      "Loss after batch 340: 0.0059; time elapsed: 4862.47\n",
      "Loss after batch 350: 0.0024; time elapsed: 4870.80\n",
      "Loss after batch 360: 0.0150; time elapsed: 4879.30\n",
      "Loss after batch 370: 0.0121; time elapsed: 4887.52\n",
      "Loss after batch 380: 0.0037; time elapsed: 4895.87\n",
      "Loss after batch 390: 0.0108; time elapsed: 4903.98\n",
      "Loss after batch 400: 0.0182; time elapsed: 4912.18\n",
      "Loss after batch 410: 0.0021; time elapsed: 4920.45\n",
      "Loss after batch 420: 0.0229; time elapsed: 4928.59\n",
      "Loss after batch 430: 0.0236; time elapsed: 4937.03\n",
      "Loss after batch 440: 0.0131; time elapsed: 4945.10\n",
      "Loss after batch 450: 0.0166; time elapsed: 4953.75\n",
      "Loss after batch 460: 0.0113; time elapsed: 4961.99\n",
      "Loss after batch 470: 0.0084; time elapsed: 4970.00\n",
      "Loss after batch 480: 0.0305; time elapsed: 4978.30\n",
      "Loss after batch 490: 0.0096; time elapsed: 4986.95\n",
      "Loss after batch 500: 0.0218; time elapsed: 4995.90\n",
      "Loss after batch 510: 0.0095; time elapsed: 5004.22\n",
      "Loss after batch 520: 0.0090; time elapsed: 5012.55\n",
      "Loss after batch 530: 0.0071; time elapsed: 5020.87\n",
      "Loss after batch 540: 0.0089; time elapsed: 5029.12\n",
      "Loss after batch 550: 0.0050; time elapsed: 5037.20\n",
      "Loss after batch 560: 0.0198; time elapsed: 5045.80\n",
      "Loss after batch 570: 0.0131; time elapsed: 5053.85\n",
      "Loss after batch 580: 0.0200; time elapsed: 5062.01\n",
      "Loss after batch 590: 0.0159; time elapsed: 5070.11\n",
      "Loss after batch 600: 0.0093; time elapsed: 5078.41\n",
      "Loss after batch 610: 0.0138; time elapsed: 5086.87\n",
      "Loss after batch 620: 0.0092; time elapsed: 5095.60\n",
      "Loss after batch 630: 0.0074; time elapsed: 5104.55\n",
      "Loss after batch 640: 0.0081; time elapsed: 5113.56\n",
      "Loss after batch 650: 0.0123; time elapsed: 5122.00\n",
      "Loss after batch 660: 0.0156; time elapsed: 5130.15\n",
      "Loss after batch 670: 0.0152; time elapsed: 5138.60\n",
      "Loss after batch 680: 0.0090; time elapsed: 5146.75\n",
      "Loss after batch 690: 0.0107; time elapsed: 5155.05\n",
      "Loss after batch 700: 0.0235; time elapsed: 5163.33\n",
      "#############################################################\n",
      "Epoch results:\n",
      "Loss train 0.023468203406082466 valid loss: 0.17316597700119019\n",
      "Validation macro average precision: 0.9523457262472529\n",
      "Epoch execution time 1166.06667304039\n",
      "#############################################################\n",
      "\n",
      "\n",
      "EPOCH 6:\n",
      "Loss after batch 10: 0.0311; time elapsed: 5735.85\n",
      "Loss after batch 20: 0.0164; time elapsed: 5743.52\n",
      "Loss after batch 30: 0.0096; time elapsed: 5751.42\n",
      "Loss after batch 40: 0.0201; time elapsed: 5759.97\n",
      "Loss after batch 50: 0.0210; time elapsed: 5768.09\n",
      "Loss after batch 60: 0.0068; time elapsed: 5775.96\n",
      "Loss after batch 70: 0.0091; time elapsed: 5784.17\n",
      "Loss after batch 80: 0.0128; time elapsed: 5792.23\n",
      "Loss after batch 90: 0.0104; time elapsed: 5800.28\n",
      "Loss after batch 100: 0.0104; time elapsed: 5808.14\n",
      "Loss after batch 110: 0.0165; time elapsed: 5816.24\n",
      "Loss after batch 120: 0.0294; time elapsed: 5824.50\n",
      "Loss after batch 130: 0.0189; time elapsed: 5832.91\n",
      "Loss after batch 140: 0.0128; time elapsed: 5841.29\n",
      "Loss after batch 150: 0.0068; time elapsed: 5849.58\n",
      "Loss after batch 160: 0.0047; time elapsed: 5857.88\n",
      "Loss after batch 170: 0.0262; time elapsed: 5865.80\n",
      "Loss after batch 180: 0.0182; time elapsed: 5873.77\n",
      "Loss after batch 190: 0.0151; time elapsed: 5881.69\n",
      "Loss after batch 200: 0.0097; time elapsed: 5889.93\n",
      "Loss after batch 210: 0.0080; time elapsed: 5898.10\n",
      "Loss after batch 220: 0.0054; time elapsed: 5906.28\n",
      "Loss after batch 230: 0.0070; time elapsed: 5914.67\n",
      "Loss after batch 240: 0.0054; time elapsed: 5922.96\n",
      "Loss after batch 280: 0.0078; time elapsed: 5956.40\n",
      "Loss after batch 290: 0.0108; time elapsed: 5965.13\n",
      "Loss after batch 300: 0.0099; time elapsed: 5973.39\n",
      "Loss after batch 310: 0.0091; time elapsed: 5981.58\n",
      "Loss after batch 320: 0.0195; time elapsed: 5989.77\n",
      "Loss after batch 330: 0.0067; time elapsed: 5998.06\n",
      "Loss after batch 340: 0.0084; time elapsed: 6006.37\n",
      "Loss after batch 350: 0.0035; time elapsed: 6014.72\n",
      "Loss after batch 360: 0.0058; time elapsed: 6022.92\n",
      "Loss after batch 370: 0.0031; time elapsed: 6031.18\n",
      "Loss after batch 380: 0.0121; time elapsed: 6039.30\n",
      "Loss after batch 390: 0.0061; time elapsed: 6047.57\n",
      "Loss after batch 400: 0.0158; time elapsed: 6055.79\n",
      "Loss after batch 410: 0.0169; time elapsed: 6064.06\n",
      "Loss after batch 420: 0.0162; time elapsed: 6072.37\n",
      "Loss after batch 430: 0.0113; time elapsed: 6080.85\n",
      "Loss after batch 440: 0.0087; time elapsed: 6088.97\n",
      "Loss after batch 450: 0.0139; time elapsed: 6097.15\n",
      "Loss after batch 460: 0.0048; time elapsed: 6105.23\n",
      "Loss after batch 470: 0.0111; time elapsed: 6113.64\n",
      "Loss after batch 480: 0.0079; time elapsed: 6122.22\n",
      "Loss after batch 490: 0.0036; time elapsed: 6130.34\n",
      "Loss after batch 500: 0.0039; time elapsed: 6138.71\n",
      "Loss after batch 510: 0.0095; time elapsed: 6146.96\n",
      "Loss after batch 520: 0.0153; time elapsed: 6155.22\n",
      "Loss after batch 530: 0.0154; time elapsed: 6163.37\n",
      "Loss after batch 540: 0.0108; time elapsed: 6171.53\n",
      "Loss after batch 550: 0.0086; time elapsed: 6179.80\n",
      "Loss after batch 560: 0.0262; time elapsed: 6188.14\n",
      "Loss after batch 570: 0.0060; time elapsed: 6196.63\n",
      "Loss after batch 580: 0.0088; time elapsed: 6205.34\n",
      "Loss after batch 590: 0.0027; time elapsed: 6213.62\n",
      "Loss after batch 600: 0.0088; time elapsed: 6221.94\n",
      "Loss after batch 610: 0.0145; time elapsed: 6230.09\n",
      "Loss after batch 620: 0.0053; time elapsed: 6238.77\n",
      "Loss after batch 630: 0.0114; time elapsed: 6247.76\n",
      "Loss after batch 640: 0.0140; time elapsed: 6256.19\n",
      "Loss after batch 650: 0.0012; time elapsed: 6264.44\n",
      "Loss after batch 660: 0.0173; time elapsed: 6272.91\n",
      "Loss after batch 670: 0.0126; time elapsed: 6281.06\n",
      "Loss after batch 680: 0.0162; time elapsed: 6289.47\n",
      "Loss after batch 690: 0.0094; time elapsed: 6297.87\n",
      "Loss after batch 700: 0.0080; time elapsed: 6306.72\n"
     ]
    }
   ],
   "source": [
    "for it in range(1):#range(5):\n",
    "        print(f\"\\n##############################################\")\n",
    "        print(f\"Iteration: {it + 1}\")\n",
    "        print(f\"##############################################\\n\")\n",
    "\n",
    "        # Model, optimizer, tensorboard setup\n",
    "        resnet = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        # Changing last classificator layer from 1000 classes to 2\n",
    "        resnet.fc = nn.Linear(512, 2)\n",
    "        # Changing 3 channels into 1 (monochromatic image)\n",
    "        resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        resnet.to(DEVICE)\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        # optimizer = torch.optim.Adam(resnet.parameters(), lr=LEARNING_RATE)\n",
    "        optimizer = torch.optim.AdamW(resnet.parameters())\n",
    "\n",
    "        writer = SummaryWriter(f'{WRITER_DIR}/Iteration_{it + 1}')\n",
    "        epoch_number = 0\n",
    "        best_vloss = sys.float_info.max\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        if not os.path.exists(MODEL_PATH + f'/Iteration_{it + 1}'):\n",
    "            os.makedirs(MODEL_PATH + f'/Iteration_{it + 1}')\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            print('EPOCH {}:'.format(epoch_number + 1))\n",
    "            epoch_start_time = time()\n",
    "\n",
    "            resnet.train(True)\n",
    "            avg_loss = train_one_epoch(epoch_number, writer, train_dl, optimizer, loss_fn, resnet, DEVICE, start_time)\n",
    "\n",
    "            resnet.eval()\n",
    "            running_vloss = 0.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, vdata in enumerate(val_dl):\n",
    "                    vinputs, vlabels = vdata\n",
    "                    voutputs = resnet(vinputs.to(DEVICE))\n",
    "                    vloss = loss_fn(voutputs, vlabels.to(DEVICE))\n",
    "                    running_vloss += vloss\n",
    "\n",
    "            avg_vloss = running_vloss / (i + 1)\n",
    "\n",
    "            print(\"#############################################################\")\n",
    "            print(\"Epoch results:\")\n",
    "            print(f'Loss train {avg_loss} valid loss: {avg_vloss}')\n",
    "            validation_precision_score = calculate_metric(resnet, val_dl, device=DEVICE,\n",
    "                                                          metric=lambda x, y: precision_score(x, y, average='macro'))\n",
    "            print(f'Validation macro average precision: {validation_precision_score}')\n",
    "            print(f'Epoch execution time {time() - epoch_start_time}')\n",
    "            print(\"#############################################################\\n\\n\")\n",
    "\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                               {'Training': avg_loss, 'Validation': avg_vloss}, epoch_number + 1)\n",
    "\n",
    "            writer.add_scalars('Macro_averaged_precision_score',\n",
    "                               {'Validation': validation_precision_score}, epoch_number + 1)\n",
    "\n",
    "            writer.flush()\n",
    "\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = f'model_{TIMESTAMP}_{epoch_number}'\n",
    "\n",
    "            torch.save(resnet.state_dict(), MODEL_PATH + f\"Iteration_{it + 1}/\" + model_path)\n",
    "\n",
    "            epoch_number += 1\n",
    "\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5d906-4bc2-4ff1-91fe-906309b8011e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 (Code Warriors)",
   "language": "python",
   "name": "code-warriors"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
